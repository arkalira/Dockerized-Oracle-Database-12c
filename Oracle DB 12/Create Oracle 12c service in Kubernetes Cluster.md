
# Create Oracle Database cluster in Kubernetes
## Creating an Oracle Database Instance Imperatively (without configuration files)
### Image from Docker Hub

https://hub.docker.com/r/sath89/oracle-12c/

- Create an Oracle Database cluster consisting of 2 replicas with container port set as 1521.

```
kubectl run oradb --image=sath89/oracle-12c --replicas=2 --port=1521
```

A replication controller called oradb gets created and a **selector (run=oradb)** to select Pods that comprise the replication controller replicas, and the number of replicas (2) get listed.

The Pod label is also set to **run=oradb**. A **pod label has to be the same as a selector expression for the Pod to be managed by the replication controller**.

- List the replication controllers.

```
kubectl get rc
```

The oradb replication controller gets listed.

- List the Pods.

```
kubectl get pods
```

Two pods (prefixed oradb) get listed for Oracle Database. Initially the Pods could be listed as not ready as indicated by the 0/1 in READY column.

Run the preceding command after a few seconds, multiple times if required, to list the two Pods as STATUS->Running and READY->(1/1).

- Listing Logs

List the logs for one of the Pods.
```
kubectl logs oradb-04hsm
```
The logs generated by a started Oracle Database instance get output.

- Describe one oradb pod

```
describe pod oradb-XXXX
```
Detailed information about the Pod such as name, namespace, Docker image, node, and labels get listed.

### Creating a Service

- Create a Kubernetes service

Creating a **replication controller does not by itself create a Docker service**. Expose the replication controller oradb as a Kubernetes service on port 1521.

```
kubectl expose rc oradb --port=1521 --type=LoadBalancer
```

The oradb service gets created. The service selector is run=oradb, which is the same as the replication controller selector.

- List the Kubernetes services.

```
kubectl get services
```

/*The service oradb gets listed.*/

- Describe the service.

```
kubectl describe svc oradb
```

The service name, namespace, labels, selector, type, IP, and endpoints get listed.

### Scaling the Database service

Run the **kubectl scale** command to scale the replicas. Scaling does not always imply increasing the number of replicas.

For example, reduce the number of replicas to 1.

```
kubectl scale rc oradb --replicas=1
```

- Subsequently list the running Pods.

```
kubectl get pods
```

/*The Pods cluster gets scaled to 1 Pod.
Only one Oracle Database Pod gets listed as the other has been stopped.*/

Subsequently, describe the service again.

```
kubectl describe svc oradb
```

/*Only one endpoint gets listed.*/


### Deleting the Replication Controller and Service

In subsequent sections we shall create a cluster of Oracle Database instances **declaratively** using definition files. **As we shall be using the same configuration parameters, delete the oradb replication controller and the oradb service**.

```
kubectl delete rc oradb
kubectl delete svc oradb
```

Both the replication controller and the service get deleted.

If the kubectl get services command is run again the oradb service does not get listed.

```
kubectl get services
```

## Creating an Oracle DB Cluster Declaratively (with configuration files)

Create definition files oradb-rc.yaml, oradb-service.yaml, and oradb.yaml as listed.

- Creating a Pod

oradb.yaml

The Pod definition file defines a Pod named “oradb” with a label setting name: “oradb”
which translates to Pod label name=oradb. The container image is set as “sath89/oracle-12c” and the container port is set as 1521.

```
apiVersion: v1
kind: Pod
metadata:
name: “oradb”
labels:
name: “oradb”
spec:
containers:
-
image: “sath89/oracle-12c”
name: “oradb”
ports:
-
containerPort: 1521
restartPolicy: Always
```

The kubectl create command is used to create a Pod from the definition file oradb.yaml.

```
kubectl create -f oradb.yaml --validate
```

An output of “pods/oradb” indicates that the oradb Pod gets created.

List the running Pods with the following command.

```
kubectl get pods
```

The single Pod oradb gets listed. The preceding command may have to be run multiple times to list the Pod as

- STATUS->Running
- READY->1/1.

Describe the Pod.

```
kubectl describe pod oradb
```

The Pod description gets listed.

### Creating a Service

 The service definition file specifies a port to expose the service at, a label for the service and a selector to match the Pods to be managed by the service. The selector setting of app: ”oradb” translates to service selector app=oradb. Copy the following listing to the oradb-service.yaml file.

```
apiVersion: v1
kind: Service
metadata:
name: "oradb"
labels:
app: "oradb"
spec:
ports:
-
port: 1521
selector:
app: "oradb"
```

Run the kubectl create command to create a service.

```
kubectl create -f oradb-service.yaml
```

The oradb service gets created. Subsequently list the services.

```
kubectl get services
```

The oradb service gets listed.

Describe the oradb service.
```
kubectl describe svc oradb
```

The service description does not include any service endpoints because the service selector does not match the label on the Pod already running.

The service selector app=oradb has to match a Pod label for the service to be able to manage the Pod.


- Creating a Replication Controller

Next, we shall create a replication controller with a matching label. The replication controller definition file called oradb-rc.yaml defines a replication controller. **For the replication controller to manage the Pods defined in the spec field the key:value expression of the selector in the replication controller has to match a label in the Pod template mapping.** The selector defaults to the same setting as the spec->template->metadata->labels. The template->spec->containers mapping defines the containers in the Pod with only the Oracle Database container "sath89/oracle-12c" defined.

```
apiVersion: v1
kind: ReplicationController
metadata:
name: "oradb"
labels:
app: "oradb"
spec:
replicas: 2
template:
metadata:
labels:
app: "oradb"
spec:
containers:
-
image: "sath89/oracle-12c"
name: "oradb"
```

Next, run the kubectl create command to create a replication controller from the definition file oradb-rc.yaml.

```
kubectl create -f oradb-rc.yaml
```

The replication controller gets created. List the replication controller.
```
kubectl get rc
```
The oradb replication controller gets created.
List the Pods created by the replication controller.

```
kubectl get pods
```

Three Oracle Database Pods get listed. Because the Pod oradb, which is started using the Pod definition file oradb.yaml does not include a label that matches the selector in the replication controller, two new replicas are started by the replication controller.

Describe the service oradb with the following command.
```
kubectl describe svc oradb
```
Service description includes name, namespace,labels,selector,type,IP, and two service endpoints.

- Scaling the Database

Next, we shall scale the cluster from 2 to 3 Pods. The kubectl scale command with replication controller is used to scale the number of Pods.
```
kubectl scale rc oradb --replicas=3
```

The Pod replicas get scaled. Subsequently list the Pods.
``
kubectl get pods
```
Three replicas of the Pod get listed. The preceding command may have to be run multiple times if required, to list the new Pod replica as running and ready.

Describe the service again.
```
kubectl describe svc oradb
```
Three endpoints should get listed in addition to a single IP address.


- Finding Pod Nodes

Previously we listed the Pods. As we are running a multi-node cluster it may be of interest to find which Pod is running on which node. To find pod->node allocation run the following command.

```
kubectl get pods –o wide
```

The Pods get listed with an additional column called NODE in the result for the node on which the pod is running.

- Starting the Interactive Shell

Next, we shall **start an interactive tty (shell) to connect to the software**, which is Oracle Database, running in a Docker container. A remote shell to a container may either be started with the **oc rsh <pod> command or docker exec -it <container> bash command**. We shall use the latter command. List the Docker containes.

```
sudo docker ps
```

Docker containers listed include those created from the sath89/oracle-12c Docker image.

Copy the container id for one of the Docker containers for the sath89/oracle-12c image and start a bash shell.

```
sudo docker exec -it c53ed6fcddf3 bash
```

A bash shell gets started. Run the **su –l oracle** command to make the user as “oracle”.

Start SQL*Plus with the **sqlplus /nolog** command. The /nolog option does not establish an initial connection with the database.

## Connecting to Database


Run the following command to connect with Oracle database instance as user SYS role SYSDBA.



CONNECT SYS AS SYSDBA



Specify the Password as “oracle” when prompted. A connection gets established.









Creating a User


Create a user called OE and grant CONNECT and RESOURCE roles to the user.



CREATE USER OE QUOTA UNLIMITED ON SYSTEM IDENTIFIED BY OE;

GRANT CONNECT, RESOURCE TO OE;



The OE user gets created and the roles get granted.







Creating a Database Table


Create a database table called OE.Catalog with the following SQL statement.



CREATE TABLE OE.Catalog(CatalogId INTEGER PRIMARY KEY,Journal VARCHAR2(25),Publisher VARCHAR2(25),Edition VARCHAR2(25),Title VARCHAR2(45),Author VARCHAR2(25));



Add a row of data to the OE.Catalog table.



INSERT INTO OE.Catalog VALUES('1','Oracle Magazine','Oracle Publishing','November December 2013','Engineering as a Service','David A. Kelly');



The OE.Catalog table gets created and a row of data gets added.







Query the OE.CATALOG table.



SELECT * FROM OE.CATALOG;



The single row of data added gets listed.





Exiting the Interactive Shell


To logout from SQL*Plus command run the “exit” command and to exit the “oracle” user run the “exit” command and to exit the interactive terminal run the “exit” command also.









In this tutorial we used Kubernetes to create and orchestrate an Oracle Database image based Pod cluster. We discussed both the imperative and declarative approaches to create and manage a cluster. We scaled the cluster and also used a Docker container to log in to SQL*Plus and create a database table.
